{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Load and Preprocess the Data"
      ],
      "metadata": {
        "id": "tphWGPSkxjC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VD_q6FeBx8SU",
        "outputId": "01193050-ab31-4de5-85b0-7847d3512bcf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BFsQnhFxPD0",
        "outputId": "9ce0630d-7634-4882-d2f3-e8c55c6597c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inspecting ../content/drive/My Drive/Train.h5:\n",
            "images\n",
            "labels\n",
            "Inspecting ../content/drive/My Drive/Test.h5:\n",
            "images\n",
            "labels\n"
          ]
        }
      ],
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "# Paths to the datasets\n",
        "train_path = \"../content/drive/My Drive/Train.h5\"\n",
        "test_path = \"../content/drive/My Drive/Test.h5\"\n",
        "\n",
        "def inspect_h5_file(file_path):\n",
        "    with h5py.File(file_path, \"r\") as f:\n",
        "        print(f\"Inspecting {file_path}:\")\n",
        "        for key in f.keys():\n",
        "            print(key)\n",
        "\n",
        "# Inspect the structure of the training and test datasets\n",
        "inspect_h5_file(train_path)\n",
        "inspect_h5_file(test_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(train_path, test_path):\n",
        "    train_dataset = h5py.File(train_path, \"r\")\n",
        "    test_dataset = h5py.File(test_path, \"r\")\n",
        "\n",
        "    train_set_x_orig = np.array(train_dataset[\"images\"][:])\n",
        "    train_set_y_orig = np.array(train_dataset[\"labels\"][:])\n",
        "\n",
        "    test_set_x_orig = np.array(test_dataset[\"images\"][:])\n",
        "    test_set_y_orig = np.array(test_dataset[\"labels\"][:])\n",
        "\n",
        "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig\n",
        "\n",
        "# Load the data\n",
        "train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig = load_data(train_path, test_path)\n",
        "\n",
        "# Inspect the data\n",
        "print(f\"Number of training examples: {train_set_x_orig.shape[0]}\")\n",
        "print(f\"Number of test examples: {test_set_x_orig.shape[0]}\")\n",
        "print(f\"Each image is of size: {train_set_x_orig.shape[1:]}\")\n",
        "\n",
        "# Flatten the images and normalize pixel values\n",
        "train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T / 255.0\n",
        "test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T / 255.0\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "num_classes = np.max(train_set_y_orig) + 1\n",
        "train_set_y = np.eye(num_classes)[train_set_y_orig].T\n",
        "test_set_y = np.eye(num_classes)[test_set_y_orig].T\n",
        "\n",
        "print(f\"Shape of training data: {train_set_x_flatten.shape}\")\n",
        "print(f\"Shape of training labels: {train_set_y.shape}\")\n",
        "print(f\"Shape of test data: {test_set_x_flatten.shape}\")\n",
        "print(f\"Shape of test labels: {test_set_y.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcVR5rGazwjz",
        "outputId": "20d5d093-5587-4394-b744-368d3583a9a6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples: 2626\n",
            "Number of test examples: 120\n",
            "Each image is of size: (128, 128, 3)\n",
            "Shape of training data: (49152, 2626)\n",
            "Shape of training labels: (5, 2626)\n",
            "Shape of test data: (49152, 120)\n",
            "Shape of test labels: (5, 120)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Implement the Softmax Activation Function"
      ],
      "metadata": {
        "id": "SEA2lSsNxlOz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(Z):\n",
        "    exp_Z = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
        "    return exp_Z / exp_Z.sum(axis=0, keepdims=True)\n"
      ],
      "metadata": {
        "id": "C8KRvdbIxYVz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Define the Cost Function for Multi-Class Classification"
      ],
      "metadata": {
        "id": "ZyuWuJX4xom7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(Y, Y_hat):\n",
        "    m = Y.shape[1]\n",
        "    cost = -1/m * np.sum(Y * np.log(Y_hat))\n",
        "    return cost\n"
      ],
      "metadata": {
        "id": "ny_93GmaxaVn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Implement the Gradient Descent Algorithm"
      ],
      "metadata": {
        "id": "um2zlgeXxq8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_parameters(dim, num_classes):\n",
        "    W = np.random.randn(num_classes, dim) * 0.01\n",
        "    b = np.zeros((num_classes, 1))\n",
        "    return W, b\n",
        "\n",
        "def forward_propagation(X, W, b):\n",
        "    Z = np.dot(W, X) + b\n",
        "    A = softmax(Z)\n",
        "    return A\n",
        "\n",
        "def backward_propagation(X, Y, A):\n",
        "    m = X.shape[1]\n",
        "    dZ = A - Y\n",
        "    dW = 1/m * np.dot(dZ, X.T)\n",
        "    db = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
        "    return dW, db\n",
        "\n",
        "def gradient_descent(X, Y, W, b, learning_rate, num_iterations):\n",
        "    for i in range(num_iterations):\n",
        "        A = forward_propagation(X, W, b)\n",
        "        cost = compute_cost(Y, A)\n",
        "\n",
        "        dW, db = backward_propagation(X, Y, A)\n",
        "\n",
        "        W = W - learning_rate * dW\n",
        "        b = b - learning_rate * db\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Cost after iteration {i}: {cost}\")\n",
        "\n",
        "    return W, b\n"
      ],
      "metadata": {
        "id": "WXKWJCuIxcRA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Test the Model and Evaluate Its Performance"
      ],
      "metadata": {
        "id": "9eR4UgANxtnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X, W, b):\n",
        "    A = forward_propagation(X, W, b)\n",
        "    predictions = np.argmax(A, axis=0)\n",
        "    return predictions\n",
        "\n",
        "def accuracy(predictions, labels):\n",
        "    return np.mean(predictions == labels) * 100\n",
        "\n",
        "# Initialize parameters\n",
        "dim = train_set_x_flatten.shape[0]\n",
        "W, b = initialize_parameters(dim, num_classes)\n",
        "\n",
        "# Train the model\n",
        "learning_rate = 0.01\n",
        "num_iterations = 1000\n",
        "W, b = gradient_descent(train_set_x_flatten, train_set_y, W, b, learning_rate, num_iterations)\n",
        "\n",
        "# Make predictions on the test set\n",
        "test_predictions = predict(test_set_x_flatten, W, b)\n",
        "test_labels = np.argmax(test_set_y, axis=0)\n",
        "\n",
        "# Evaluate accuracy\n",
        "test_accuracy = accuracy(test_predictions, test_labels)\n",
        "print(f\"Test accuracy: {test_accuracy}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7VSPRFCxejG",
        "outputId": "89f9c471-3e4c-425c-9707-b8e24a0a9edd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost after iteration 0: 2.003216846469892\n",
            "Cost after iteration 100: 10.093959022192664\n",
            "Cost after iteration 200: 14.807001810690807\n",
            "Cost after iteration 300: 9.227468477681176\n",
            "Cost after iteration 400: 10.282358025499647\n",
            "Cost after iteration 500: 11.804511658595276\n",
            "Cost after iteration 600: 10.091224340709891\n",
            "Cost after iteration 700: 7.937442895515193\n",
            "Cost after iteration 800: 10.287739863600018\n",
            "Cost after iteration 900: 4.343463055757876\n",
            "Test accuracy: 36.666666666666664%\n"
          ]
        }
      ]
    }
  ]
}