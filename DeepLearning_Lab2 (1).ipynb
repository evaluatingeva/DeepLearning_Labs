{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Eva Saini 1RVU22CSE053"
      ],
      "metadata": {
        "id": "JyCr9MhmS1az"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EDo8CfNQvJM",
        "outputId": "26fdd176-ed40-4991-8881-5890f718e6d5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step1: Load and preprocess the data."
      ],
      "metadata": {
        "id": "IK6cu3QxS5rK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tHZe2vYtOxIi"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_file_path = '/content/drive/My Drive/Colab Notebooks/Tr.h5'\n",
        "test_file_path = '/content/drive/My Drive/Colab Notebooks/Te.h5'\n",
        "\n",
        "# Load the dataset\n",
        "def load_data():\n",
        "   train_dataset = h5py.File(train_file_path, \"r\")\n",
        "   test_dataset = h5py.File(test_file_path, \"r\")\n",
        "\n",
        "    # Extract images and labels\n",
        "   train_X = np.array(train_dataset[\"images\"][:])  # training set features\n",
        "   train_Y = np.array(train_dataset[\"labels\"][:])  # training set labels\n",
        "   test_X = np.array(test_dataset[\"images\"][:])    # test set features\n",
        "   test_Y = np.array(test_dataset[\"labels\"][:])\n",
        "\n",
        "   return train_X, train_Y, test_X, test_Y\n",
        "\n",
        "# Preprocessing the data (flattening and normalizing)\n",
        "def preprocess_data(train_X, test_X):\n",
        "    train_X_flatten = train_X.reshape(train_X.shape[0], -1).T  # Flatten images\n",
        "    test_X_flatten = test_X.reshape(test_X.shape[0], -1).T\n",
        "\n",
        "    # Normalize pixel values to [0, 1]\n",
        "    train_X_norm = train_X_flatten / 255.0\n",
        "    test_X_norm = test_X_flatten / 255.0\n",
        "\n",
        "    return train_X_norm, test_X_norm\n",
        "\n",
        "# One-hot encode labels for multi-class classification\n",
        "def one_hot_encode(labels, num_classes):\n",
        "    return np.eye(num_classes)[labels.reshape(-1)].T\n",
        "\n",
        "train_X, train_Y, test_X, test_Y = load_data()\n",
        "train_X, test_X = preprocess_data(train_X, test_X)\n",
        "train_Y_one_hot = one_hot_encode(train_Y, 5)\n",
        "test_Y_one_hot = one_hot_encode(test_Y, 5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Implement the softmax activation function."
      ],
      "metadata": {
        "id": "c2Q7mbVVTDnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(Z):\n",
        "    expZ = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
        "    return expZ / np.sum(expZ, axis=0, keepdims=True)"
      ],
      "metadata": {
        "id": "9scEn7PyO_jK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Define the cost function for multi-class classification."
      ],
      "metadata": {
        "id": "tXdx2wvaTIhE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(AL, Y):\n",
        "    m = Y.shape[1]\n",
        "    cost = -np.sum(Y * np.log(AL)) / m\n",
        "    cost = np.squeeze(cost)  # To make sure cost is a scalar\n",
        "    return cost\n"
      ],
      "metadata": {
        "id": "cuM19RzgPCk9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4:Implement the gradient descent algorithm to update the model parameters."
      ],
      "metadata": {
        "id": "LYKeTSv8TT96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_parameters(layer_dims):\n",
        "    np.random.seed(1)\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)  # Number of layers\n",
        "\n",
        "    for l in range(1, L):\n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
        "\n",
        "    return parameters\n"
      ],
      "metadata": {
        "id": "pl4_rYlcPQTG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_forward(A, W, b):\n",
        "    Z = np.dot(W, A) + b\n",
        "    return Z\n",
        "\n",
        "def forward_propagation(X, parameters):\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) // 2  # Number of layers\n",
        "\n",
        "    for l in range(1, L):\n",
        "        A_prev = A\n",
        "        W = parameters['W' + str(l)]\n",
        "        b = parameters['b' + str(l)]\n",
        "        Z = linear_forward(A_prev, W, b)\n",
        "        A = np.maximum(0, Z)  # ReLU activation\n",
        "        caches.append((A_prev, W, b, Z))\n",
        "\n",
        "    # Final layer (Softmax activation)\n",
        "    W_L = parameters['W' + str(L)]\n",
        "    b_L = parameters['b' + str(L)]\n",
        "    ZL = linear_forward(A, W_L, b_L)\n",
        "    AL = softmax(ZL)\n",
        "\n",
        "    caches.append((A, W_L, b_L, ZL))\n",
        "    return AL, caches\n"
      ],
      "metadata": {
        "id": "y-thr95oPSrI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_backward(dZ, cache):\n",
        "    A_prev, W, b, Z = cache\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "    dW = np.dot(dZ, A_prev.T) / m\n",
        "    db = np.sum(dZ, axis=1, keepdims=True) / m\n",
        "    dA_prev = np.dot(W.T, dZ)\n",
        "\n",
        "    return dA_prev, dW, db\n",
        "\n",
        "def backward_propagation(AL, Y, caches):\n",
        "    grads = {}\n",
        "    L = len(caches)\n",
        "    m = AL.shape[1]\n",
        "\n",
        "    # Initial gradient (for Softmax)\n",
        "    dZL = AL - Y\n",
        "    current_cache = caches[L - 1]\n",
        "    grads[\"dA\" + str(L - 1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_backward(dZL, current_cache)\n",
        "\n",
        "    for l in reversed(range(L - 1)):\n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = linear_backward(grads[\"dA\" + str(l + 1)], current_cache)\n",
        "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "\n",
        "    return grads\n"
      ],
      "metadata": {
        "id": "jULYCq63PV5j"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    L = len(parameters) // 2  # Number of layers\n",
        "\n",
        "    for l in range(1, L + 1):\n",
        "        parameters[\"W\" + str(l)] -= learning_rate * grads[\"dW\" + str(l)]\n",
        "        parameters[\"b\" + str(l)] -= learning_rate * grads[\"db\" + str(l)]\n",
        "\n",
        "    return parameters\n"
      ],
      "metadata": {
        "id": "D49yKMElPYbT"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Test the model on the test set and evaluate its performance."
      ],
      "metadata": {
        "id": "NY6oQcexTXey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model(X, Y, layer_dims, learning_rate=0.0075, num_iterations=3000):\n",
        "    np.random.seed(1)\n",
        "    costs = []\n",
        "    parameters = initialize_parameters(layer_dims)\n",
        "\n",
        "    for i in range(0, num_iterations):\n",
        "        AL, caches = forward_propagation(X, parameters)\n",
        "        cost = compute_cost(AL, Y)\n",
        "        grads = backward_propagation(AL, Y, caches)\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "            print(f\"Cost after iteration {i}: {cost}\")\n",
        "\n",
        "    return parameters\n"
      ],
      "metadata": {
        "id": "AjZXOYkjPad2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X, parameters):\n",
        "    AL, _ = forward_propagation(X, parameters)\n",
        "    return np.argmax(AL, axis=0)\n",
        "\n",
        "# Evaluate on the test set\n",
        "def evaluate(test_X, test_Y, parameters):\n",
        "    predictions = predict(test_X, parameters)\n",
        "    accuracy = np.mean(predictions == np.argmax(test_Y, axis=0))\n",
        "    print(f\"Test Accuracy: {accuracy * 100}%\")\n",
        "\n",
        "# Updated layer dimensions\n",
        "parameters = model(train_X, train_Y_one_hot, layer_dims=[49152, 20, 7, 5], num_iterations=2500)\n",
        "evaluate(test_X, test_Y_one_hot, parameters)\n"
      ],
      "metadata": {
        "id": "IEe_6TO8Pc6K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8585654a-1abb-4140-c57c-24e14d9f715a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost after iteration 0: 1.6093863775118022\n",
            "Cost after iteration 100: 1.6093653349174797\n",
            "Cost after iteration 200: 1.6093399737891814\n",
            "Cost after iteration 300: 1.6092981584337163\n",
            "Cost after iteration 400: 1.6092316177357309\n",
            "Cost after iteration 500: 1.6091209622689981\n",
            "Cost after iteration 600: 1.608928251370577\n",
            "Cost after iteration 700: 1.6085769747318805\n",
            "Cost after iteration 800: 1.6078870659265754\n",
            "Cost after iteration 900: 1.6063252638690704\n",
            "Cost after iteration 1000: 1.6021459807370393\n",
            "Cost after iteration 1100: 1.5893718178581249\n",
            "Cost after iteration 1200: 1.5649568358787882\n",
            "Cost after iteration 1300: 1.5519777552492608\n",
            "Cost after iteration 1400: 1.5298897239217712\n",
            "Cost after iteration 1500: 1.4264337948610597\n",
            "Cost after iteration 1600: 1.3484281277406864\n",
            "Cost after iteration 1700: 1.2931289615132975\n",
            "Cost after iteration 1800: 1.2448921706040106\n",
            "Cost after iteration 1900: 1.1870354733613868\n",
            "Cost after iteration 2000: 1.1298179162970359\n",
            "Cost after iteration 2100: 1.0647812067414812\n",
            "Cost after iteration 2200: 0.9827845499312653\n",
            "Cost after iteration 2300: 0.8568932234585966\n",
            "Cost after iteration 2400: 0.7508404110687056\n",
            "Test Accuracy: 34.0%\n"
          ]
        }
      ]
    }
  ]
}